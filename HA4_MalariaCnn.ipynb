{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HA4_MalariaCnn.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDRcJq6lZpSP"
      },
      "source": [
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih9RBNUijfjc"
      },
      "source": [
        "We load the datset and split it into test and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEjOsRwZtjK"
      },
      "source": [
        "#alternativ data_test, data_train = tfds.load('Malaria', as_supervised = True, split = [\"train[:10%]\", \"train[10%:100%]\"])\n",
        "data_test, info_test = tfds.load('Malaria', as_supervised = True, split = \"train[:10%]\", with_info=True)\n",
        "data_train = tfds.load('Malaria',as_supervised = True, split = \"train[10%:100%]\")\n",
        "#data has shape (<tf.Tensor: shape=(103, 103, 3), dtype=uint8, numpy= array\n",
        "# if not as_Supervised {'image': <tf.Tensor: shape=(142, 151, 3), dtype=uint8, numpy=array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grduocY2apP1"
      },
      "source": [
        "#split the dataset into training and test data\n",
        "\n",
        "# Num classes: 2\n",
        "print(\"Num classes: \" + str(info_test.features['label'].num_classes))\n",
        "#Class names: ['parasitized', 'uninfected']\n",
        "print(\"Class names: \" + str(info_test.features['label'].names))\n",
        "\n",
        "vis = tfds.visualization.show_examples(data_test, info_test)\n",
        "\n",
        "# gc.collect()  = garbage collector to regain capacities\n",
        "\n",
        "for image, label in data_train.take(1):\n",
        "    print(\"Image shape: \", image.numpy().shape)\n",
        "    print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxIBoDvjgq1d"
      },
      "source": [
        "**Preprocessing**\n",
        "\n",
        "Our images have different sizes but we don't want an accidental correlation between image size and label. Also, it is more convenient to have all data in the same shape when feeding it to the network. Therefore we use padding to make all images the same size, by adding some surrounding pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-hGc6Y9gpG1"
      },
      "source": [
        "# these are the functions we use to pad\n",
        "\n",
        "#def convert(image, label):\n",
        "#  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "#  return image, label\n",
        "\n",
        "#Crops and/or pads an image to a target width and height\n",
        "def pad(image,label):\n",
        "  image,label = convert(image, label)\n",
        "  image = tf.image.resize_with_crop_or_pad(image, 200, 200)\n",
        "  return image,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cw-tin7r92B"
      },
      "source": [
        "# here we apply our pad function by using the map function\n",
        "# while were at it we also bath\n",
        "\n",
        "padded_data_train = (\n",
        "    data_train\n",
        "    .cache()\n",
        "    .map(pad)\n",
        "    .batch(128) # we use minibatches instead of sibgle samples to be more efficient\n",
        ") \n",
        "\n",
        "padded_data_test = (\n",
        "    data_test\n",
        "    .cache()\n",
        "    .map(pad)\n",
        "    .batch(128)\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xQCK3AIsmdN"
      },
      "source": [
        "vis = tfds.visualization.show_examples(padded_data_train, info_test) #thisonly works if you comment out the batching above\n",
        "#here we find out the shape of the image, so that we can use it tp pass it to ur model\n",
        "# (Image shape:  (200, 200, 3))\n",
        "for image, label in padded_data_train.take(1):\n",
        "    print(\"Image shape: \", image.numpy().shape)\n",
        "    print(\"Label: \", label.numpy())\n",
        "    print(label)\n",
        "    label = tf.one_hot(label, depth=2)\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsHZ2ykKtxgf"
      },
      "source": [
        "**Model**\n",
        "\n",
        "Lets build a model for our convolutional neural network.\n",
        "We need convolutional and pooling layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAD7XfJKs2qi"
      },
      "source": [
        "def conv_block(filters):\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPool2D()\n",
        "    ]\n",
        "    )\n",
        "    \n",
        "    return block\n",
        "\n",
        "def dense_block(units, dropout_rate):\n",
        "    block = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    \n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E116zMOxa2PS"
      },
      "source": [
        "#functions from opencast\n",
        "# tf.keras.layers.MaxPool2D(\n",
        "#    pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs\n",
        "#)\n",
        "\n",
        "#tf.keras.layers.GlobalAveragePooling2D(\n",
        "#    data_format=None, **kwargs\n",
        "#)\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# You will implement a CNN. The layers you need are: tf.keras.layers.Conv2D, tf.keras.layers.MaxPool2D, \n",
        "# tf.keras.layers.Flatten, tf.keras.layers.Dense\n",
        "\n",
        "# Input\n",
        "# Convolutional layer: 32 kernels of size (3,3) with ReLU activation function (tf.keras.activations.relu)\n",
        "# Note: In the first layer you have to provide the input shape to the function (argument: input_shape).\n",
        "# Maxpooling layer: pooling size (2,2) and strides (2,2).\n",
        "# Convolutional layer: 64 kernels of size (3,3) with ReLU activation function \n",
        "# Maxpooling layer: pooling size (2,2) and strides (2,2).\n",
        "# Convolutional layer: 64 kernels of size (3,3) with ReLU activation function\n",
        "# Flatten the resulting feature maps.\n",
        "# Fully connected layer with 64 hidden neurons and ReLU activation function.\n",
        "# Fully connected layer with 10 output neurons and softmax activation function (tf.keras.activations.softmax).\n",
        "\n",
        "# INitialize model\n",
        "class Model(Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # Initialize all layers.\n",
        "        # we alternate between convolutional and pooling layers\n",
        "        #because this is the first convolutional layer we have to pass the input shape to it\n",
        "        self.conv_layer_1 = tf.keras.layers.Conv2D(\n",
        "                                filters=32,\n",
        "                                kernel_size=3,\n",
        "                                activation=tf.keras.activations.relu,\n",
        "                                #input shape = shape of images plus extra dimension because of batching\n",
        "                                input_shape=(200,200,3,1) \n",
        "                            )\n",
        "        # our pooling layer, we use max-pooling\n",
        "        self.max_pool_1 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_layer_2 = tf.keras.layers.Conv2D(\n",
        "                                filters=64,\n",
        "                                kernel_size=3,\n",
        "                                activation=tf.keras.activations.relu\n",
        "                            )\n",
        "        self.max_pool_2 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_layer_3 = tf.keras.layers.Conv2D(\n",
        "                                filters=64,\n",
        "                                kernel_size=3,\n",
        "                                activation=tf.keras.activations.relu\n",
        "                            )\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fully_connected_1 = tf.keras.layers.Dense(\n",
        "                                units=64,\n",
        "                                activation=tf.keras.activations.relu\n",
        "                            )\n",
        "        # for the last layer we use a fully connected layer with sotmax activation function. Outputs are between 0 and 1 and add up to 1\n",
        "        self.fully_connected_2 = tf.keras.layers.Dense(\n",
        "                                units=10,\n",
        "                                activation=tf.keras.activations.softmax\n",
        "                            )\n",
        "    \n",
        "    # here the model gets called and the input gets passed through all the layers  \n",
        "    def call(self, x):\n",
        "        # Define the model.\n",
        "        x = self.conv_layer_1(x)\n",
        "        x = self.max_pool_1(x)\n",
        "        x = self.conv_layer_2(x)\n",
        "        x = self.max_pool_2(x)\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fully_connected_1(x)\n",
        "        x = self.fully_connected_2(x)\n",
        "        return x\n",
        "        ######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SayPqFSZhDAg"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lR2aiKLhFsy"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Initialize model, loss (binary cross entropy) and optimizer (Adam).\n",
        "model = Model()\n",
        "# we use the binary corss entropy to computethe loss because we have two labels (infected, uninfected)\n",
        "binary_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Initialize lists for later visualiztion\n",
        "train_steps = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "step = 0\n",
        "\n",
        "\n",
        "for epoch in range(3):\n",
        "    #t = target\n",
        "    for (x,t) in padded_data_train:\n",
        "        \n",
        "        t = tf.reshape(t, shape=[-1])\n",
        "        \n",
        "        ### YOUR CODE HERE ###\n",
        "        # Turn the labels into one-hot vectors.\n",
        "        t = tf.one_hot(t, depth=10) #why depth 10? can we make it work with depth =2 siince we have 2 labels?\n",
        "        \n",
        "        # Perform a training step.\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(x)\n",
        "            loss = binary_entropy_loss(t, output)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
        "        \n",
        "        # Calculate the training accuracy every 25 steps.\n",
        "        if step % 25 == 0:\n",
        "            accuracy = np.sum(np.argmax(t, axis=1) == np.argmax(output, axis=1)) / t.shape[0]\n",
        "            train_accuracies.append(accuracy)\n",
        "            train_losses.append(loss)\n",
        "            train_steps.append(step)\n",
        "        \n",
        "        # Calculate the test loss and accuracy every 50 steps.\n",
        "        if step % 50 == 0:\n",
        "            for (x,t) in padded_data_test:\n",
        "                t = tf.reshape(t, shape=[-1])\n",
        "                t = tf.one_hot(t, depth=10)\n",
        "                output = model(x)\n",
        "                loss = binary_entropy_loss(t, output)\n",
        "                accuracy = np.sum(np.argmax(t, axis=1) == np.argmax(output, axis=1)) / t.shape[0]\n",
        "                test_steps.append(step)\n",
        "                test_accuracies.append(accuracy)\n",
        "                test_losses.append(loss)\n",
        "        ########################\n",
        "        \n",
        "        step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn0bvhVhjgiY"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_VRoDEKjh9x"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#goal: 95% accuracy\n",
        "\n",
        "# Visualize accuracy and loss for training and test data. \n",
        "#Plot training and test loss.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_steps, train_losses)\n",
        "line2, = plt.plot(test_steps, test_losses)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\n",
        "plt.show()\n",
        "\n",
        "# Plot training and test accuracy.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_steps, train_accuracies)\n",
        "line2, = plt.plot(test_steps, test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\n",
        "plt.show()\n",
        "#####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ORSL2HyPHE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}