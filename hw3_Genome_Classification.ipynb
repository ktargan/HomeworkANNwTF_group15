{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Genome_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aypP6S5sKfMQ"
      },
      "source": [
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Sv_-9itW_M"
      },
      "source": [
        "Function for turning tensor string into a one-hot vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxtiXO5VZQus"
      },
      "source": [
        "def string_to_one_hot(strg):\n",
        "  vocab = {'A':'1', 'C': '2', 'G':'3', 'T':'0'}\n",
        "  \n",
        "  for key in vocab.keys(): #'ACTG' #'1230'\n",
        "    strg = tf.strings.regex_replace(strg, key, rewrite = vocab[key])\n",
        "    #strg = tf.strings.regex_replace(strg, pattern = 'A', rewrite = 1, replace_global = True)\n",
        "  #strg = tf.strings.regex_replace(strg, pattern = \"C\", rewrite = 2, replace_global = True)\n",
        "  #strg = tf.strings.regex_replace(strg, pattern = 'T', rewrite = 3, replace_global = True\n",
        "  split = tf.strings.bytes_split(strg)\n",
        "  labels = tf.cast(tf.strings.to_number(split), tf.uint8)\n",
        "  onehot = tf.one_hot(labels, 4)\n",
        "  onehot = tf.reshape(onehot, (-1,)) #e.g. [0001]\n",
        "  return onehot"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMJLx1-ULxin"
      },
      "source": [
        "Load the Data-Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsQO5nbwr5BO"
      },
      "source": [
        "test_data, training_data = tfds.load('genomics_ood', as_supervised = True, split = ['test[0:1000]', 'train[0:100000]'])\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8GheLh9KoI-"
      },
      "source": [
        "\n",
        "training_data = training_data.map(lambda seq, label: (string_to_one_hot(seq), tf.one_hot(label, 10)))\n",
        "training_data = training_data.batch(128)\n",
        "training_data = training_data.shuffle(buffer_size=128)\n",
        "training_data = training_data.prefetch(20)\n",
        "\n",
        "test_data = test_data.map(lambda seq, label: (string_to_one_hot(seq), tf.one_hot(label, 10)))\n",
        "test_data = test_data.batch(128)\n",
        "test_data = test_data.shuffle(buffer_size=128)\n",
        "test_data = test_data.prefetch(20)\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLzw-EbhzOcx"
      },
      "source": [
        "The Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-v0G5rrzP_K"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Model(Model):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "\n",
        "    self.hidden_layer_1 = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.sigmoid)\n",
        "    self.hidden_layer_2 = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.sigmoid)\n",
        "    self.output_layer = tf.keras.layers.Dense(units=10, activation = tf.keras.activations.softmax)\n",
        "\n",
        "  def call(self, input):\n",
        "    input = self.hidden_layer_1(input)\n",
        "    input = self.hidden_layer_2(input)\n",
        "    input = self.output_layer(input)\n",
        "    return input"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bLWIwBm12tg"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h98S9Ipf15pG"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  #write a custom training loop\n",
        "  #allows for automatic differentiation: automatically computes the derivative of a function\n",
        "  #by repeatedly applying the chain rule\n",
        "  with tf.GradientTape() as tape:\n",
        "    #first: make a prediction based on model and loss\n",
        "    prediction = model(input)\n",
        "    #compute the loss given the prediction and the target\n",
        "    loss = loss_function(prediction, target)\n",
        "    #now we need the partial deriatives of the loss with respect to all the weights\n",
        "    #this is where gradienttape comes in handy and allows for easy computation of all the partial derivatives\n",
        "    gradients = tape.gradient(loss, model.training_variables)\n",
        "  #last we apply the computed weight updates\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  #to be able to take record of the error produced by our network we return the loss\n",
        "  return loss"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyWds2EsugDk"
      },
      "source": [
        "Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjTvD06kuioG"
      },
      "source": [
        "'''Testing the performance of our model. By computing the loss and '''\n",
        "def test(model, test_data, loss_function):\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    computed_loss = loss_function(prediction, target)\n",
        "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_losses.append(computed_loss.numpy())\n",
        "    test_accuracies.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  \n",
        "  loss = np.mean(test_losses)\n",
        "  test_accuracy = np.mean(test_accuracies)\n",
        "  return loss, test_accuracy\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVZzA7NVuP4i"
      },
      "source": [
        "Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqBYybUyuO5-",
        "outputId": "9c5a963b-01e4-4f92-ea31-aa4be8a8c5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "#tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model\n",
        "model = Model()\n",
        "\n",
        "#hyperparameters\n",
        "epochs = 10\n",
        "learning_rate = 0.1 \n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "running_average_factor = 0.95\n",
        "\n",
        "#To keep track of the processes, we use several lists\n",
        "training_steps = []\n",
        "train_losses= []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#test once before training the model\n",
        "test_loss, test_accuracy = test(model, training_data, loss)\n",
        "\n",
        "#how does the model do on training data before training?\n",
        "train_loss = test(model, training_data, loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "#now start to train\n",
        "for epoch in range(epochs):\n",
        "  training_steps.append(epoch)\n",
        "  #run through the current batch\n",
        "\n",
        "  training_data = training_data.shuffle(buffer_size=128)\n",
        "  test_data = test_data.shuffle(buffer_size=128)\n",
        "\n",
        "  running_average = 0\n",
        "  for i,(input, label) in enumerate(training_dataset):\n",
        "    training_loss = test_step(model, input, label, loss, optimizer)\n",
        "\n",
        "    #record how the loss evolves over one epoch\n",
        "    running_average = running_average_factor * running_average  + (1 - running_average_factor) * train_loss \n",
        "\n",
        "  training_losses.append(running_average)\n",
        "\n",
        "  #now evaluate the model performance on test set\n",
        "  test_loss, test_accur = test(test_data, loss, optimizer)\n",
        "\n",
        "  test_losses.append(test_loss)\n",
        "  test_accuracies.append(test_accur)\n",
        "\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-5a79aa52aa73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mrunning_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alSwg5nc198X"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-ix7rpT1_Mm"
      },
      "source": [
        "#do the visualization\n",
        "#test loss and training loss\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FijhoZxRsEfV"
      },
      "source": [
        "#test accuracy\n",
        "plt.figure()\n",
        "plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}