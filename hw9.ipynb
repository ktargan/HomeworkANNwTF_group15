{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSR_FYdPvz7o"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to99YNHiyUl6"
      },
      "source": [
        "#generate the dataset\r\n",
        "def generate_data(numb_samples, seq_length):\r\n",
        "  targets = [tf.one_hot(np.random.randint(low=0, high=10, size = 2),10) for i in range(numb_samples)]\r\n",
        "  sequences = [tf.one_hot(np.random.randint(low=0,high=10, size= seq_length),10) for i in range(numb_samples)]\r\n",
        "  \r\n",
        "  ground_truth = []\r\n",
        "  for target,seq in zip(targets,sequences):\r\n",
        "    ground_truth.append(calculate_target(targets, sequences))\r\n",
        "\r\n",
        "  sequences = tf.data.Dataset.from_tensor_slices(sequences)\r\n",
        "  targets = tf.data.Dataset.from_tensor_slices(targets)\r\n",
        "  ground_truth = tf.data_Dataset.from_tensor_slices(ground_truth)\r\n",
        "\r\n",
        "  dataset = tf.data.Dataset.zip((sequences, targets, ground_truth))\r\n",
        "\r\n",
        "  dataset = dataset.batch(16)\r\n",
        "  dataset = dataset.shuffle(16)\r\n",
        "  dataset = dataset.prefetch(8)\r\n",
        "\r\n",
        "  print(dataset)\r\n",
        "  return dataset"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz3A128tcCCo"
      },
      "source": [
        "def calculate_target(target_options, sequence):\r\n",
        "  first_target = np.sum(sequence == target_options[0])\r\n",
        "  second_target = np.sum(sequence == target_options[1])\r\n",
        "\r\n",
        "  #if the first target number appears more often or equally often as the second we will return 0\r\n",
        "  larger_sum = 0\r\n",
        "  if first_target < second_target:\r\n",
        "    larger_sum = 1\r\n",
        "\r\n",
        "  return larger_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPtqvt-ePnTm"
      },
      "source": [
        "#LSTM Cell\r\n",
        "class LSTMCell(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, units):\r\n",
        "    super(LSTMCell, self).__init__()\r\n",
        "\r\n",
        "    self.cell_state = tf.zeros(shape= (BATCHSIZE, units=500))\r\n",
        "    self.hidden_state = tf.zeros(shape = (BATCHSIZE, units=500))\r\n",
        "\r\n",
        "    #gates as dense layers with respective activation function\r\n",
        "    self.forget_gate = tf.keras.layers.Dense(units = 2 , activation = tf.keras.activations.sigmoid, bias_initializer='ones')\r\n",
        "    self.input_gate = tf.keras.layers.Dense(units = 2, activation = tf.keras.activations.sigmoid)\r\n",
        "    self.output_gate = tf.keras.layers.Dense(units = 2, activation = tf.keras.activations.sigmoid)\r\n",
        "    \r\n",
        "    #cell state candidates with tanh\r\n",
        "    self.candidates = tf.keras.layers.Dense(units = 2, activation = tf.keras.activations.tanh)\r\n",
        "\r\n",
        "    self.tanh = tf.keras.layers.tanh()\r\n",
        "\r\n",
        "  def call(self, input,hidden_state, cell_state):\r\n",
        "    #concatenate input and hidden state\r\n",
        "    concat_input = tf.concat(hidden_state,input)\r\n",
        "\r\n",
        "    #forget part of cell state\r\n",
        "    cell_state = cell_state * self.forget_gate(concat_input)\r\n",
        "\r\n",
        "    #compute update for cell state\r\n",
        "    update = self.input_gate(concat_input) * self.candidates(concat_input)\r\n",
        "    #use input gate and candidates to update cell state\r\n",
        "    cell_state = cell_state + update\r\n",
        "\r\n",
        "    #compute new hidden state\r\n",
        "    hidden_state = ouput_gate(concat_input) * self.tanh(cell_State)\r\n",
        "\r\n",
        "    #ouput the hidden state\r\n",
        "    return hidden_state"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bXqIRSHP-et"
      },
      "source": [
        "class LSTM(tf.keras.layers.Layer):\r\n",
        "  def __init__(self):\r\n",
        "    super(LSTM, self).__init__()\r\n",
        "    self.read_in = \r\n",
        "    self.cell = LSTMCell()\r\n",
        "   \r\n",
        "  def call(self, x):\r\n",
        "    #inititialize cell_state\r\n",
        "    cell_state = tf.zeros()\r\n",
        "    hidden_state = tf.zeros()\r\n",
        "\r\n",
        "    #sequence and query concatenation\r\n",
        "    hidden_state, cell_state = self.cell(x, hidden_state, cell_state)\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5RK4JGixdNB"
      },
      "source": [
        "class Model(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Model,self).__init__()\r\n",
        "    self.input_layer = #?\r\n",
        "    self.lstm = LSTM()\r\n",
        "    self.flatten = tf.keras.layers.Flatten()\r\n",
        "    #output which of the target numbers is more likely\r\n",
        "    self.read_out = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.softmax)\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x = self.input_layer(x)\r\n",
        "    x = self.flatten(x)\r\n",
        "    x = self.lstm(x)\r\n",
        "    x = self.read_out(x)\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unt9CmIoUUXX"
      },
      "source": [
        "def train_step(model, data, queries, target, loss_function, optimizer):\r\n",
        "  \r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    prediction = model(data,queries)\r\n",
        "\r\n",
        "    #calculate the loss as a sum of the SGD to minimize prediction error and l2 regularization (penalize large weights)\r\n",
        "    loss = loss_function(prediction, target)\r\n",
        "    #calculate the accuracy by moving along the vector of targets (per input)\r\n",
        "    # comparing the correct target value to the class with the highest prediction\r\n",
        "    accuracy = np.sum(np.argmax(target, axis=1) == np.argmax(prediction, axis=1)) / target.shape[0]\r\n",
        "    #calculate the gradients for the weights with respect to the loss\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "  \r\n",
        "  #update the weights\r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "  return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31inC9dfUMwr"
      },
      "source": [
        "#testing a batch\r\n",
        "def test(model, data, loss_function):\r\n",
        "  test_losses = []\r\n",
        "  test_accuracies = []\r\n",
        "\r\n",
        "  #iterate over image, label tuples in the batch\r\n",
        "  for (input,queries, target) in data:\r\n",
        "    #compute the prediction for the model (forward pass)\r\n",
        "    prediction = model(input,queries)\r\n",
        "    #compute the loss of the model with the loss function \r\n",
        "    loss = loss_function(prediction,target)\r\n",
        "    \r\n",
        "    #compare the real to the predicted label \r\n",
        "    #the predicted label is the category with the highest probability\r\n",
        "    accuracy = (np.argmax(prediction, axis=1) == np.argmax(target,axis=1))\r\n",
        "  \r\n",
        "    #add the computed values to the aggregation lists\r\n",
        "    test_losses.append(loss.numpy())\r\n",
        "    #before adding the accuracy we take it's mean to \r\n",
        "    test_accuracies.append(np.mean(accuracy))\r\n",
        "\r\n",
        "  t_loss = np.mean(test_losses)\r\n",
        "  t_accuracy = np.mean(test_accuracies)\r\n",
        "\r\n",
        "  return t_loss, t_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "znqyJuXwc2R9",
        "outputId": "8e9c126d-766c-4b16-e252-a3a863fd46db"
      },
      "source": [
        "#train\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "#create data\r\n",
        "train_data = generate_data(60000,30)\r\n",
        "test_data = generate_data(1000,30)\r\n",
        "\r\n",
        "#model\r\n",
        "model = Model()\r\n",
        "\r\n",
        "#Define hyperparameters\r\n",
        "#How many training epochs do we perform\r\n",
        "epochs = 30\r\n",
        "#define the learning rate which influences the magnitude with which we update the models parameters\r\n",
        "learning_rate = 0.001\r\n",
        "loss_function = tf.keras.losses.BinaryCrossEntropy()\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
        "\r\n",
        "running_average_factor = 0.95\r\n",
        "\r\n",
        "steps = []\r\n",
        "train_losses = []\r\n",
        "train_accuracy = []\r\n",
        "test_losses = []\r\n",
        "test_accuracy = []\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "  print(epoch, \". epoch --------------------------------------------------------------------------------\")\r\n",
        "  steps.append(epoch)\r\n",
        "\r\n",
        "\r\n",
        "  start = time.time()\r\n",
        "  original_loss = 0\r\n",
        "  for data,queries,target in train_data:\r\n",
        "    train_loss,train_accuracy = train_step(model, data, queries, target, loss_function, optimizer)\r\n",
        "\r\n",
        "    original_loss = running_average_factor * original_loss + (1-running_average_factor) * train_loss\r\n",
        "    original_acc = running_average_factor * original_acc + (1-running_average_factor) * train_accuracy\r\n",
        "\r\n",
        "  train_losses.append(original_loss)\r\n",
        "  train_accuracy.append(original_acc)\r\n",
        "\r\n",
        "  test_loss,test_accuracy = test(model,test_data, loss_function)\r\n",
        "  test_losses.append(test_loss)\r\n",
        "  test_accuracies.append(test_accuracy)\r\n",
        "\r\n",
        "  print(f\"the training step and test evaluation took {timing(start)} seconds\")\r\n",
        "  #some sort of plotting\r\n",
        "  print(\"train_loss\", np.mean(train_losses))\r\n",
        "  print(\"test_loss\", test_loss.numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((None, 50, 10), (None, 2, 10)), types: (tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1614c2407f47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q63t-jDYTqY8"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "#do the visualization\r\n",
        "#test loss and training loss\r\n",
        "plt.figure()\r\n",
        "line1, = plt.plot(train_losses)\r\n",
        "line2, = plt.plot(test_losses)\r\n",
        "plt.xlabel(\"Training steps\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#test accuracy and training accuracy\r\n",
        "plt.figure()\r\n",
        "line1, = plt.plot(train_accuracies)\r\n",
        "line2, = plt.plot(test_accuracies)\r\n",
        "plt.xlabel(\"Training steps\")\r\n",
        "plt.ylabel(\"Test/Training accuracy\")\r\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}